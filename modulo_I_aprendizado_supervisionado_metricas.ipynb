{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4IGnz3i3GSz9wjFYe445z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prof-eduardo-galego/fiap-bpm-ia/blob/main/modulo_I_aprendizado_supervisionado_metricas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo I - Aprendizado Supervisionado - Métricas\n",
        "\n",
        "Neste notebook vamos conhecer alguns dos tipos de métricas mais utilizados aplicados à um modelo.\n",
        "\n",
        "Este exemplo foi adaptado de https://keytodatascience.com/confusion-matrix/\n",
        "\n",
        "## Biblioteca e Módulos\n",
        "\n",
        "Vamos utilizar a bilioteca __Scikit Learn__ para este exemplo. Segue código para carregá-la com os módulos que utilizaremos ao longo do notebook:"
      ],
      "metadata": {
        "id": "byiQN6OaHxaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEDRRjeVHrvc"
      },
      "outputs": [],
      "source": [
        " # Python script for confusion matrix creation.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dados\n",
        "\n",
        "Neste exemplo vamos considerar uma base de dados pequena, com dados reais e dados que nosso modelo retornou (repare que alguns dados retornados pelo modelo não batem com o observado). Segue:"
      ],
      "metadata": {
        "id": "cmCccZ4fIzBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual = ['dog','cat', 'dog', 'cat', 'dog', 'dog', 'cat', 'dog', 'dog', 'cat']\n",
        "predicted = ['dog', 'dog', 'dog', 'cat', 'dog', 'dog', 'cat', 'cat', 'cat', 'cat']"
      ],
      "metadata": {
        "id": "Lid1EhAXJGTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de Confusão\n",
        "\n",
        "Vamos utilizar o módulo `confusion_matriz` para construir nossa matriz de confusão, informando os dados do modelo e os observador. Segue:"
      ],
      "metadata": {
        "id": "xyq_XNuAJGoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = confusion_matrix(actual, predicted)\n",
        "print ('Matriz de confusão :')\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA4WLpluJg93",
        "outputId": "336ed0ac-94de-4572-e8fd-ed4754690eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de confusão :\n",
            "[[3 1]\n",
            " [2 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas\n",
        "\n",
        "Vamos utilizar os módulo `accuracy_score` e `classification_report` para calcular as métricas:"
      ],
      "metadata": {
        "id": "4jzaWtM7JhdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Acurácia:',accuracy_score(actual, predicted))\n",
        "print()\n",
        "print('Outras métricas: ')\n",
        "print(classification_report(actual, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ENt4keLJwQY",
        "outputId": "25e529fb-7f99-4145-f34b-7542c6505bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.7\n",
            "\n",
            "Outras métricas: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         cat       0.60      0.75      0.67         4\n",
            "         dog       0.80      0.67      0.73         6\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.70      0.71      0.70        10\n",
            "weighted avg       0.72      0.70      0.70        10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumo\n",
        "\n",
        "A métrica __Acurária__ determina o quanto o seu modelo acertou na previsões.\n",
        "\n",
        "Já a métrica __Precisão__ determinad o quão certo seu modelo está dos __verdadeiros positivos__. E a métrica __Recall__ é o quanto você tem certeza de que não está perdendo nenhum ponto positivo.\n",
        "\n",
        "__*Mas quais destas métricas considerar na interpretação de algum resultado?*__\n",
        "\n",
        "* Escolha __Recall__ se a ocorrência de falsos negativos for inaceitável/intolerável. Por exemplo: No caso de diabetes, você prefere ter alguns falsos positivos extras (falsos alarmes) em vez de salvar alguns falsos negativos.\n",
        "\n",
        "* Escolha __Precisão__ se quiser ter mais confiança em seus verdadeiros positivos. Por exemplo: No caso de e-mails de spam, você prefere ter alguns e-mails de spam em sua caixa de entrada em vez de alguns e-mails normais em sua caixa de spam. *Você gostaria de ter certeza de que o e-mail X é spam antes de colocá-lo na caixa de spam.*\n",
        "\n",
        "* Escolha __Especificidade__ se quiser cobrir todos os verdadeiros negativos, ou seja, significa que não queremos nenhum alarme falso ou falso positivo. Por exemplo: No caso de um teste de drogas em que todas as pessoas com resultado positivo irão imediatamente para a cadeia, você não gostaria que ninguém livre de drogas fosse para a cadeia."
      ],
      "metadata": {
        "id": "y1omyA68JoC0"
      }
    }
  ]
}